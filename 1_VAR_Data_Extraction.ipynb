{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VAR Fairness Audit: Data Extraction\n\n**DS 112 Final Project**\n\nThis notebook focuses on loading and preparing the VAR datasets for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n!pip install pandas numpy matplotlib seaborn plotly scikit-learn scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set plotting style\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading\n\nFirst, we'll load the VAR incident and team stats datasets. We'll explore multiple loading methods and handle potential file access issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define file paths\nVAR_INCIDENTS_FILE = 'VAR_Incidents_Stats.csv'\nTEAM_STATS_FILE = 'VAR_Team_Stats.csv'\nOUTPUT_FILE = 'var_combined.csv'\n\n# Function to safely load CSV data\ndef safe_load_csv(file_path, description):\n    try:\n        data = pd.read_csv(file_path)\n        print(f\"\u2705 Successfully loaded {description} data with shape: {data.shape}\")\n        return data\n    except FileNotFoundError:\n        print(f\"\u274c Error: Could not find {file_path}\")\n        print(f\"Make sure the {description} file is in the current working directory.\")\n        return None\n    except pd.errors.EmptyDataError:\n        print(f\"\u274c Error: The file {file_path} is empty.\")\n        return None\n    except pd.errors.ParserError:\n        print(f\"\u274c Error: The file {file_path} could not be parsed as CSV.\")\n        return None\n    except Exception as e:\n        print(f\"\u274c Error loading {file_path}: {str(e)}\")\n        return None\n\n# Load the datasets\nvar_incidents = safe_load_csv(VAR_INCIDENTS_FILE, \"VAR incidents\")\nteam_stats = safe_load_csv(TEAM_STATS_FILE, \"team stats\")\n\n# Check if both datasets were loaded successfully\nif var_incidents is not None and team_stats is not None:\n    print(\"\\nBoth datasets loaded successfully!\")\nelse:\n    print(\"\\n\u26a0\ufe0f Warning: One or both datasets failed to load.\")\n    print(\"Some code below may not work properly.\")\n\n# Display basic information about the datasets\nif var_incidents is not None:\n    print(\"\\nVAR Incidents Dataset Info:\")\n    print(f\"- Rows: {var_incidents.shape[0]}\")\n    print(f\"- Columns: {var_incidents.shape[1]}\")\n    print(\"\\nVAR Incidents Dataset - First 5 rows:\")\n    display(var_incidents.head())\n\nif team_stats is not None:\n    print(\"\\nTeam Stats Dataset Info:\")\n    print(f\"- Rows: {team_stats.shape[0]}\")\n    print(f\"- Columns: {team_stats.shape[1]}\")\n    print(\"\\nTeam Stats Dataset - First 5 rows:\")\n    display(team_stats.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Understanding\n\nLet's examine the structure and content of both datasets in detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine the column names and data types\nif var_incidents is not None:\n    print(\"VAR Incidents Dataset - Column Information:\")\n    print(var_incidents.info())\n    print(\"\\nColumn Descriptions:\")\n    for col in var_incidents.columns:\n        print(f\"- {col}\")\n\nif team_stats is not None:\n    print(\"\\nTeam Stats Dataset - Column Information:\")\n    print(team_stats.info())\n    print(\"\\nColumn Descriptions:\")\n    for col in team_stats.columns:\n        print(f\"- {col}\")\n\n# Check for unique values in categorical columns (if VAR incidents exists)\nif var_incidents is not None:\n    print(\"\\nUnique values in key VAR incident columns:\")\n    categorical_cols = ['team_name', 'decision_type', 'match_minute', 'season']\n    \n    for col in categorical_cols:\n        if col in var_incidents.columns:\n            print(f\"\\n{col} - {var_incidents[col].nunique()} unique values:\")\n            print(var_incidents[col].value_counts().head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning\n\nNext, we'll clean the datasets by handling missing values, standardizing formats, and addressing any inconsistencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to analyze missing values\ndef analyze_missing_values(df, dataset_name):\n    print(f\"\\n{dataset_name} - Missing Value Analysis:\")\n    missing = df.isnull().sum()\n    missing_percent = (missing / len(df)) * 100\n    missing_df = pd.DataFrame({'Missing Values': missing, 'Percentage': missing_percent})\n    missing_df = missing_df[missing_df['Missing Values'] > 0].sort_values('Missing Values', ascending=False)\n    \n    if len(missing_df) > 0:\n        print(missing_df)\n    else:\n        print(\"No missing values found!\")\n    return missing_df\n\n# Analyze missing values in both datasets\nif var_incidents is not None:\n    var_missing = analyze_missing_values(var_incidents, \"VAR Incidents Dataset\")\n\nif team_stats is not None:\n    team_missing = analyze_missing_values(team_stats, \"Team Stats Dataset\")\n\n# Function to clean dataset\ndef clean_dataset(df, dataset_name):\n    print(f\"\\nCleaning {dataset_name}...\")\n    # Make a copy to avoid modifying the original\n    cleaned_df = df.copy()\n    \n    # 1. Handle missing values\n    for col in cleaned_df.columns:\n        missing_count = cleaned_df[col].isnull().sum()\n        if missing_count > 0:\n            print(f\"- Column '{col}' has {missing_count} missing values\")\n            \n            # Determine data type and handle accordingly\n            if pd.api.types.is_numeric_dtype(cleaned_df[col]):\n                # Fill numeric columns with median\n                median_val = cleaned_df[col].median()\n                cleaned_df[col].fillna(median_val, inplace=True)\n                print(f\"  \u2713 Filled with median value: {median_val}\")\n            else:\n                # Fill categorical columns with mode\n                mode_val = cleaned_df[col].mode()[0]\n                cleaned_df[col].fillna(mode_val, inplace=True)\n                print(f\"  \u2713 Filled with most common value: {mode_val}\")\n    \n    # 2. Standardize text fields (lowercase for text columns)\n    for col in cleaned_df.columns:\n        if cleaned_df[col].dtype == 'object':\n            try:\n                # Convert to lowercase if it's a string column\n                cleaned_df[col] = cleaned_df[col].str.strip().str.lower()\n                print(f\"- Standardized text format in column '{col}'\")\n            except:\n                print(f\"- Could not standardize column '{col}'\")\n    \n    # 3. Remove duplicate rows\n    dupes = cleaned_df.duplicated().sum()\n    if dupes > 0:\n        print(f\"- Found {dupes} duplicate rows\")\n        cleaned_df.drop_duplicates(inplace=True)\n        print(f\"  \u2713 Removed all duplicate rows\")\n    else:\n        print(\"- No duplicate rows found\")\n    \n    print(f\"\u2705 Cleaned {dataset_name} successfully!\")\n    return cleaned_df\n\n# Clean the datasets\nif var_incidents is not None:\n    cleaned_incidents = clean_dataset(var_incidents, \"VAR Incidents Dataset\")\nelse:\n    cleaned_incidents = None\n\nif team_stats is not None:\n    cleaned_team_stats = clean_dataset(team_stats, \"Team Stats Dataset\")\nelse:\n    cleaned_team_stats = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering\n\nLet's add some useful derived features before merging the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering for VAR incidents\nif cleaned_incidents is not None:\n    print(\"Adding derived features to VAR incidents dataset...\")\n    \n    # 1. Match time period (group match minutes into periods)\n    if 'match_minute' in cleaned_incidents.columns:\n        time_bins = [0, 15, 30, 45, 60, 75, 90, 120]\n        time_labels = ['0-15', '16-30', '31-45', '46-60', '61-75', '76-90', '90+']\n        cleaned_incidents['time_period'] = pd.cut(cleaned_incidents['match_minute'], \n                                                bins=time_bins, \n                                                labels=time_labels, \n                                                right=False)\n        print(\"\u2713 Added 'time_period' based on match minute\")\n    \n    # 2. Decision outcome - was it favorable to the team?\n    if 'decision_type' in cleaned_incidents.columns:\n        # Define favorable decisions (this is an assumption, adjust as needed)\n        favorable_decisions = ['penalty_awarded', 'goal_allowed', 'red_card_to_opponent']\n        unfavorable_decisions = ['penalty_overturned', 'goal_disallowed', 'red_card_to_team']\n        \n        # Create a decision outcome feature\n        def determine_favorability(decision):\n            if decision in favorable_decisions:\n                return 1  # Favorable\n            elif decision in unfavorable_decisions:\n                return 0  # Unfavorable\n            else:\n                return 0.5  # Neutral\n        \n        cleaned_incidents['decision_favorable'] = cleaned_incidents['decision_type'].apply(determine_favorability)\n        print(\"\u2713 Added 'decision_favorable' feature\")\n    \n    # 3. Count decisions by team\n    team_decision_counts = cleaned_incidents.groupby('team_name').size().reset_index(name='total_var_decisions')\n    cleaned_incidents = pd.merge(cleaned_incidents, team_decision_counts, on='team_name', how='left')\n    print(\"\u2713 Added 'total_var_decisions' by team\")\n    \n    # Display the new features\n    print(\"\\nVAR Incidents with new features:\")\n    display(cleaned_incidents.head())\n\n# Feature engineering for team stats\nif cleaned_team_stats is not None:\n    print(\"\\nAdding derived features to team stats dataset...\")\n    \n    # 1. Team tier based on ranking (if rank column exists)\n    rank_column = next((col for col in cleaned_team_stats.columns if 'rank' in col.lower()), None)\n    if rank_column:\n        # Create team tiers (quartiles)\n        cleaned_team_stats['team_tier'] = pd.qcut(cleaned_team_stats[rank_column], \n                                                q=4, \n                                                labels=['Top Tier', 'Upper Mid', 'Lower Mid', 'Bottom Tier'])\n        print(f\"\u2713 Added 'team_tier' based on {rank_column}\")\n    \n    # Display the new features\n    print(\"\\nTeam Stats with new features:\")\n    display(cleaned_team_stats.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Merging\n\nNow we'll merge the two datasets to create a comprehensive dataset for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge the cleaned datasets\nif cleaned_incidents is not None and cleaned_team_stats is not None:\n    print(\"Merging VAR incidents and team stats datasets...\")\n    \n    # Identify the common key for merging\n    team_name_col_incidents = 'team_name'\n    team_name_col_stats = 'team_name'\n    \n    # Check if team names match between datasets\n    incidents_teams = set(cleaned_incidents[team_name_col_incidents].unique())\n    stats_teams = set(cleaned_team_stats[team_name_col_stats].unique())\n    \n    common_teams = incidents_teams.intersection(stats_teams)\n    incidents_only = incidents_teams - stats_teams\n    stats_only = stats_teams - incidents_teams\n    \n    print(f\"Teams in both datasets: {len(common_teams)}\")\n    print(f\"Teams only in incidents: {len(incidents_only)}\")\n    print(f\"Teams only in stats: {len(stats_only)}\")\n    \n    if len(incidents_only) > 0:\n        print(\"\\nTeams in incidents but not in stats:\")\n        print(list(incidents_only)[:5]), # Show only first 5 if many\n    \n    # Perform the merge\n    merged_df = pd.merge(cleaned_incidents, cleaned_team_stats, \n                        left_on=team_name_col_incidents, \n                        right_on=team_name_col_stats, \n                        how='left')\n    \n    # Check for successful merge\n    print(f\"\\nMerged dataset shape: {merged_df.shape}\")\n    print(f\"Original incidents shape: {cleaned_incidents.shape}\")\n    \n    # Count rows where team stats data is missing\n    missing_stats = merged_df[merged_df.iloc[:, cleaned_incidents.shape[1]:].isnull().all(axis=1)].shape[0]\n    print(f\"Incidents without team stats: {missing_stats} ({missing_stats/len(merged_df)*100:.1f}%)\")\n    \n    # Display the merged dataset\n    print(\"\\nMerged Dataset - First 5 rows:\")\n    display(merged_df.head())\n    \n    # Save the merged dataset\n    merged_df.to_csv(OUTPUT_FILE, index=False)\n    print(f\"\\n\u2705 Merged dataset saved to '{OUTPUT_FILE}'\")\nelse:\n    print(\"\\n\u274c Could not merge datasets because one or both datasets are missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Validation\n\nLet's verify the quality of our merged dataset before proceeding to analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the saved merged dataset to verify it was saved correctly\ntry:\n    verified_df = pd.read_csv(OUTPUT_FILE)\n    print(f\"Successfully loaded the merged dataset from {OUTPUT_FILE}\")\n    print(f\"Dataset shape: {verified_df.shape}\")\n    \n    # Basic validation checks\n    print(\"\\nRunning validation checks...\")\n    \n    # 1. Check for missing values\n    missing = verified_df.isnull().sum()\n    missing_cols = missing[missing > 0]\n    if len(missing_cols) > 0:\n        print(\"\\nColumns with missing values:\")\n        print(missing_cols)\n    else:\n        print(\"\u2713 No missing values found\")\n    \n    # 2. Check for duplicates\n    dupes = verified_df.duplicated().sum()\n    if dupes > 0:\n        print(f\"\u26a0\ufe0f Found {dupes} duplicate rows in the merged dataset\")\n    else:\n        print(\"\u2713 No duplicate rows found\")\n    \n    # 3. Check column data types\n    print(\"\\nColumn data types:\")\n    print(verified_df.dtypes)\n    \n    # 4. Basic statistics for numerical columns\n    print(\"\\nBasic statistics for numerical columns:\")\n    print(verified_df.describe())\n    \n    print(\"\\n\u2705 Data validation complete!\")\n    print(\"The dataset is ready for exploratory analysis and modeling.\")\n    \nexcept Exception as e:\n    print(f\"\u274c Error validating the merged dataset: {str(e)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}