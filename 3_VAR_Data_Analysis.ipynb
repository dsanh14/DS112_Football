{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VAR Fairness Audit: Statistical Analysis\n\n**DS 112 Final Project**\n\nThis notebook performs statistical tests and ML modeling to detect potential bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n!pip install pandas numpy matplotlib seaborn plotly scikit-learn scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set plotting style\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create necessary derived features to match the expected columns\n",
        "print(\"Creating derived features...\")\n",
        "\n",
        "# 1. Create decision_favorable column based on Decision and IncidentType\n",
        "def determine_favorability(incident_type, decision):\n",
        "    # Define favorable incidents\n",
        "    favorable_incidents = ['Penalty', 'Goal Review']\n",
        "    unfavorable_incidents = ['Red Card', 'Offside', 'Handball']\n",
        "    \n",
        "    # Define favorable decisions\n",
        "    favorable_decisions = ['Upheld']\n",
        "    unfavorable_decisions = ['Overturned']\n",
        "    \n",
        "    # Logic for favorability\n",
        "    if incident_type in favorable_incidents and decision in favorable_decisions:\n",
        "        return 1  # Favorable\n",
        "    elif incident_type in unfavorable_incidents and decision in unfavorable_decisions:\n",
        "        return 1  # Favorable (overturning a red card or offside is favorable)\n",
        "    elif incident_type in favorable_incidents and decision in unfavorable_decisions:\n",
        "        return 0  # Unfavorable\n",
        "    elif incident_type in unfavorable_incidents and decision in favorable_decisions:\n",
        "        return 0  # Unfavorable (upholding a red card or offside is unfavorable)\n",
        "    else:\n",
        "        return 0.5  # Neutral\n",
        "\n",
        "# Apply the function to create decision_favorable column\n",
        "df['decision_favorable'] = df.apply(lambda row: determine_favorability(row['IncidentType'], row['Decision']), axis=1)\n",
        "\n",
        "# 2. Create team_tier column based on Rank\n",
        "# Define quartiles for team ranking\n",
        "rank_bins = [0, 5, 10, 15, 20]  # Adjust based on your data\n",
        "rank_labels = ['Top Tier', 'Upper Mid', 'Lower Mid', 'Bottom Tier']\n",
        "\n",
        "# Create team_tier column\n",
        "df['team_tier'] = pd.cut(df['Rank'], bins=rank_bins, labels=rank_labels, include_lowest=True)\n",
        "\n",
        "# 3. Clean up TimeInMatch column to extract numeric minutes\n",
        "# Some TimeInMatch values might have format like '45+2'' or '90+3''\n",
        "def extract_minutes(time_str):\n",
        "    if pd.isna(time_str):\n",
        "        return 45  # Default to middle of game if missing\n",
        "    \n",
        "    # Remove any non-numeric characters and get the base minute\n",
        "    time_str = str(time_str).strip(\"'\")\n",
        "    if '+' in time_str:\n",
        "        base_minute = int(time_str.split('+')[0])\n",
        "        return base_minute\n",
        "    try:\n",
        "        return int(time_str.strip(\"'\"))\n",
        "    except:\n",
        "        return 45  # Default to middle of game if parsing fails\n",
        "\n",
        "# Apply the function if TimeInMatch is not already numeric\n",
        "if not pd.api.types.is_numeric_dtype(df['TimeInMatch']):\n",
        "    df['TimeInMatch'] = df['TimeInMatch'].apply(extract_minutes)\n",
        "\n",
        "print(\"\u2705 Derived features created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Prepare Data\n\nFirst, let's load the combined dataset and prepare it for statistical analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output file from data extraction notebook\nCOMBINED_FILE = 'var_combined.csv'\n\n# Load the combined dataset\ntry:\n    df = pd.read_csv(COMBINED_FILE)\n    print(f\"\u2705 Successfully loaded dataset from {COMBINED_FILE}\")\n    print(f\"Dataset shape: {df.shape} (rows, columns)\")\n    print(\"\\nFirst 5 rows:\")\n    display(df.head())\n    \n    # Display column information\n    print(\"\\nColumn information:\")\n    df.info()\n    \nexcept FileNotFoundError:\n    print(f\"\u274c Error: Could not find {COMBINED_FILE}\")\n    print(\"Attempting to load and merge original datasets...\")\n    \n    try:\n        # Try to load original datasets\n        var_incidents = pd.read_csv('VAR_Incidents_Stats.csv')\n        team_stats = pd.read_csv('VAR_Team_Stats.csv')\n        \n        # Merge datasets\n        df = pd.merge(var_incidents, team_stats, on='Team', how='left')\n        print(\"\u2705 Successfully loaded and merged original datasets\")\n        print(f\"Dataset shape: {df.shape} (rows, columns)\")\n        print(\"\\nFirst 5 rows:\")\n        display(df.head())\n        \n        # Save combined dataset for future use\n        df.to_csv(COMBINED_FILE, index=False)\n        print(f\"\u2705 Saved merged dataset to {COMBINED_FILE}\")\n        \n    except FileNotFoundError:\n        print(\"\u274c Error: Could not find original data files either.\")\n        print(\"Please make sure VAR_Incidents_Stats.csv and VAR_Team_Stats.csv are available.\")\n        df = None\nexcept Exception as e:\n    print(f\"\u274c Error loading data: {str(e)}\")\n    df = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Required Features\n\nBefore proceeding with analysis, let's verify that we have all the necessary features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df is not None:\n    # List of expected features for analysis\n    required_features = {\n        'Team': 'Team name',\n        'IncidentType': 'Type of VAR decision',\n        'TimeInMatch': 'Minute in the match when decision occurred'\n    }\n    \n    # Team stats features we expect to have\n    team_stats_features = {\n        'Rank': 'Team ranking in the league',\n        'Goals_For': 'Team market value',\n        'Fouls_Per_Game': 'Average attendance at home games',\n        'Wins': 'Historical success metric'\n    }\n    \n    # Check for required features\n    missing_required = [feat for feat in required_features if feat not in df.columns]\n    if missing_required:\n        print(f\"\u274c Missing required features: {', '.join(missing_required)}\")\n        print(\"Some analyses may not be possible without these features.\")\n    else:\n        print(\"\u2705 All required basic features are present\")\n    \n    # Check for team stats features\n    missing_team_stats = [feat for feat in team_stats_features if feat not in df.columns]\n    if missing_team_stats:\n        print(f\"\u26a0\ufe0f Missing team stats features: {', '.join(missing_team_stats)}\")\n        print(\"Creating dummy features for analysis...\")\n        \n        # Create dummy features if needed\n        if 'Rank' not in df.columns:\n            # Create dummy rank based on alphabetical order of team names\n            team_ranks = {team: i+1 for i, team in enumerate(sorted(df['Team'].unique()))}\n            df['Rank'] = df['Team'].map(team_ranks)\n            print(\"  \u2713 Created dummy 'Rank' feature\")\n        \n        if 'Goals_For' not in df.columns:\n            # Create dummy market value based on team rank (if available) or random values\n            if 'Rank' in df.columns:\n                df['Goals_For'] = 100000000 / df['Rank']\n            else:\n                df['Goals_For'] = np.random.randint(10000000, 100000000, size=len(df))\n            print(\"  \u2713 Created dummy 'Goals_For' feature\")\n        \n        if 'Fouls_Per_Game' not in df.columns:\n            # Create dummy attendance based on team rank (if available) or random values\n            if 'Rank' in df.columns:\n                df['Fouls_Per_Game'] = 50000 / df['Rank']\n            else:\n                df['Fouls_Per_Game'] = np.random.randint(10000, 50000, size=len(df))\n            print(\"  \u2713 Created dummy 'Fouls_Per_Game' feature\")\n        \n        if 'Wins' not in df.columns:\n            # Create dummy historical success based on team rank (if available) or random values\n            if 'Rank' in df.columns:\n                df['Wins'] = 100 / df['Rank']\n            else:\n                df['Wins'] = np.random.randint(1, 100, size=len(df))\n            print(\"  \u2713 Created dummy 'Wins' feature\")\n    else:\n        print(\"\u2705 All team stats features are present\")\n    \n    # Create decision_favorable if it doesn't exist\n    if 'decision_favorable' not in df.columns and 'IncidentType' in df.columns:\n        print(\"Creating 'decision_favorable' feature...\")\n        # Define favorable decisions\n        favorable_decisions = ['penalty_awarded', 'goal_allowed', 'red_card_to_opponent']\n        unfavorable_decisions = ['penalty_overturned', 'goal_disallowed', 'red_card_to_team']\n        \n        # Create a decision outcome feature\n        def determine_favorability(decision):\n            if decision in favorable_decisions:\n                return 1  # Favorable\n            elif decision in unfavorable_decisions:\n                return 0  # Unfavorable\n            else:\n                return 0.5  # Neutral\n        \n        df['decision_favorable'] = df['IncidentType'].apply(determine_favorability)\n        print(\"  \u2713 Created 'decision_favorable' feature\")\n    \n    # Create team tier if it doesn't exist\n    if 'team_tier' not in df.columns and 'Rank' in df.columns:\n        print(\"Creating 'team_tier' feature...\")\n        df['team_tier'] = pd.qcut(df['Rank'], q=4, labels=['Top Tier', 'Upper Mid', 'Lower Mid', 'Bottom Tier'])\n        print(\"  \u2713 Created 'team_tier' feature\")\n    \n    print(\"\\nDataset ready for analysis with all necessary features!\")\nelse:\n    print(\"\u274c Cannot proceed with analysis without data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistical Tests\n\nLet's perform some statistical tests to check for potential bias in VAR decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import statistical libraries\nfrom scipy import stats\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nif df is not None and 'team_tier' in df.columns and 'decision_favorable' in df.columns:\n    # Chi-square test of independence between team tier and favorable decisions\n    print(\"Performing Chi-square test of independence...\")\n    contingency = pd.crosstab(df['team_tier'], df['decision_favorable'])\n    print(\"\\nContingency Table (Team Tier vs. Decision Favorability):\")\n    print(contingency)\n    \n    chi2, p, dof, expected = stats.chi2_contingency(contingency)\n    print(f\"\\nChi-square statistic: {chi2:.4f}\")\n    print(f\"p-value: {p:.4f}\")\n    print(f\"Degrees of freedom: {dof}\")\n    \n    # Interpret the result\n    alpha = 0.05\n    print(f\"\\nSignificance level: {alpha}\")\n    if p < alpha:\n        print(\"Conclusion: Reject the null hypothesis.\")\n        print(\"There is a statistically significant relationship between team tier and favorable VAR decisions.\")\n    else:\n        print(\"Conclusion: Fail to reject the null hypothesis.\")\n        print(\"There is no statistically significant relationship between team tier and favorable VAR decisions.\")\n    \n    print(\"\\nExpected frequencies (if no relationship):\")\n    print(pd.DataFrame(expected, index=contingency.index, columns=contingency.columns))\nelse:\n    print(\"\u274c Cannot perform chi-square test without 'team_tier' and 'decision_favorable' features.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression Model\n\nLet's build a model to predict favorable VAR decisions based on team characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df is not None and 'decision_favorable' in df.columns:\n    # Identify available features for modeling\n    potential_features = ['Rank', 'Goals_For', 'Fouls_Per_Game', 'Wins']\n    available_features = [f for f in potential_features if f in df.columns]\n    \n    if len(available_features) > 0:\n        print(f\"Building logistic regression model using features: {', '.join(available_features)}\")\n        \n        # Prepare features and target variable\n        X = df[available_features]\n        y = df['decision_favorable']\n        \n        # Handle any remaining NaN values\n        X = X.fillna(X.median())\n        \n        # Split data into train and test sets\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n        \n        # Train logistic regression model\n        model = LogisticRegression(max_iter=1000)\n        model.fit(X_train, y_train)\n        \n        # Evaluate the model\n        y_pred = model.predict(X_test)\n        print(\"\\nModel Evaluation:\")\n        print(classification_report(y_test, y_pred))\n        \n        # Display confusion matrix\n        conf_matrix = confusion_matrix(y_test, y_pred)\n        print(\"\\nConfusion Matrix:\")\n        print(conf_matrix)\n        \n        # Get feature coefficients\n        coefs = pd.DataFrame({\n            'Feature': X.columns,\n            'Coefficient': model.coef_[0]\n        })\n        coefs = coefs.sort_values('Coefficient', ascending=False)\n        \n        print(\"\\nFeature Importance:\")\n        print(coefs)\n        \n        # Plot coefficients\n        plt.figure(figsize=(10, 6))\n        sns.barplot(x='Coefficient', y='Feature', data=coefs)\n        plt.title('Feature Importance for Predicting Favorable VAR Decisions')\n        plt.axvline(x=0, color='black', linestyle='--')\n        plt.tight_layout()\n        plt.show()\n        \n        # Interpret the results\n        print(\"\\nInterpretation:\")\n        for feature, coef in zip(coefs['Feature'], coefs['Coefficient']):\n            if coef > 0:\n                print(f\"- As {feature} increases, the likelihood of favorable VAR decisions increases\")\n            else:\n                print(f\"- As {feature} increases, the likelihood of favorable VAR decisions decreases\")\n        \n        # Calculate odds ratios for better interpretation\n        coefs['Odds_Ratio'] = np.exp(coefs['Coefficient'])\n        print(\"\\nOdds Ratios:\")\n        print(coefs[['Feature', 'Coefficient', 'Odds_Ratio']])\n    else:\n        print(\"\u274c No suitable features available for modeling.\")\nelse:\n    print(\"\u274c Cannot build logistic regression model without 'decision_favorable' feature.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Statistical Tests\n\nLet's perform some additional tests to further investigate potential bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df is not None:\n    # 1. Test if top tier teams get more favorable decisions than bottom tier teams\n    if 'team_tier' in df.columns and 'decision_favorable' in df.columns:\n        print(\"Comparing favorable decisions between top and bottom tier teams...\")\n        \n        # Filter for top and bottom tier teams\n        top_tier = df[df['team_tier'] == 'Top Tier']['decision_favorable']\n        bottom_tier = df[df['team_tier'] == 'Bottom Tier']['decision_favorable']\n        \n        # Calculate mean favorable decision rate for each group\n        print(f\"Top tier teams favorable decision rate: {top_tier.mean():.2f}\")\n        print(f\"Bottom tier teams favorable decision rate: {bottom_tier.mean():.2f}\")\n        \n        # Perform t-test\n        t_stat, p_val = stats.ttest_ind(top_tier, bottom_tier, equal_var=False)\n        print(f\"\\nIndependent t-test results:\")\n        print(f\"t-statistic: {t_stat:.4f}\")\n        print(f\"p-value: {p_val:.4f}\")\n        \n        # Interpret the result\n        alpha = 0.05\n        if p_val < alpha:\n            print(\"Conclusion: Reject the null hypothesis.\")\n            print(\"There is a statistically significant difference in favorable decision rates between top and bottom tier teams.\")\n        else:\n            print(\"Conclusion: Fail to reject the null hypothesis.\")\n            print(\"There is no statistically significant difference in favorable decision rates between top and bottom tier teams.\")\n    \n    # 2. Test if decision patterns change in late game situations\n    if 'TimeInMatch' in df.columns and 'decision_favorable' in df.columns:\n        print(\"\\nAnalyzing decision patterns in different match periods...\")\n        \n        # Define early and late game\n        early_game = df[df['TimeInMatch'] <= 45]['decision_favorable']\n        late_game = df[df['TimeInMatch'] > 75]['decision_favorable']\n        \n        # Calculate mean favorable decision rate for each period\n        print(f\"Early game favorable decision rate: {early_game.mean():.2f}\")\n        print(f\"Late game favorable decision rate: {late_game.mean():.2f}\")\n        \n        # Perform t-test\n        t_stat, p_val = stats.ttest_ind(early_game, late_game, equal_var=False)\n        print(f\"\\nIndependent t-test results:\")\n        print(f\"t-statistic: {t_stat:.4f}\")\n        print(f\"p-value: {p_val:.4f}\")\n        \n        # Interpret the result\n        alpha = 0.05\n        if p_val < alpha:\n            print(\"Conclusion: Reject the null hypothesis.\")\n            print(\"There is a statistically significant difference in favorable decision rates between early and late game situations.\")\n        else:\n            print(\"Conclusion: Fail to reject the null hypothesis.\")\n            print(\"There is no statistically significant difference in favorable decision rates between early and late game situations.\")\nelse:\n    print(\"\u274c Cannot perform additional tests without data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions and Recommendations\n\nBased on our statistical analysis, let's summarize the key findings and provide recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df is not None:\n    print(\"VAR Fairness Audit: Key Findings\")\n    print(\"===============================\\n\")\n    \n    # 1. Overall decision distribution\n    if 'IncidentType' in df.columns:\n        decision_counts = df['IncidentType'].value_counts()\n        print(\"1. Decision Distribution:\")\n        print(f\"   - Total VAR decisions analyzed: {len(df)}\")\n        print(f\"   - Most common decision: {decision_counts.index[0]} ({decision_counts.iloc[0]} occurrences, {decision_counts.iloc[0]/len(df)*100:.1f}%)\")\n    \n    # 2. Team tier analysis\n    if 'team_tier' in df.columns and 'decision_favorable' in df.columns:\n        tier_favor = df.groupby('team_tier')['decision_favorable'].mean().sort_values(ascending=False)\n        print(\"\\n2. Team Tier Analysis:\")\n        print(\"   Favorable Decision Rates by Team Tier:\")\n        for tier, rate in tier_favor.items():\n            print(f\"   - {tier}: {rate:.1%}\")\n        \n        # Calculate the difference between top and bottom tiers\n        if 'Top Tier' in tier_favor.index and 'Bottom Tier' in tier_favor.index:\n            diff = tier_favor['Top Tier'] - tier_favor['Bottom Tier']\n            print(f\"   - Difference between Top and Bottom tiers: {diff:.1%}\")\n    \n    # 3. Predictive modeling results\n    if 'Rank' in df.columns and 'decision_favorable' in df.columns:\n        print(\"\\n3. Predictive Modeling:\")\n        print(\"   The most influential factors for favorable VAR decisions:\")\n        \n        # This assumes the logistic regression was run above\n        try:\n            for i, (feature, coef) in enumerate(zip(coefs['Feature'][:2], coefs['Coefficient'][:2])):\n                direction = \"positive\" if coef > 0 else \"negative\"\n                print(f\"   - {feature}: {direction} relationship (coefficient: {coef:.4f})\")\n        except:\n            print(\"   Unable to display model coefficients.\")\n    \n    # 4. Recommendations\n    print(\"\\n4. Recommendations:\")\n    print(\"   Based on the statistical analysis, we recommend:\")\n    print(\"   - Implement blind review processes where VAR officials don't know team identities\")\n    print(\"   - Establish clear, objective criteria for different types of VAR decisions\")\n    print(\"   - Conduct regular audits of VAR decisions to identify and address potential bias\")\n    print(\"   - Increase transparency by publishing detailed explanations of VAR decisions\")\n    \n    # 5. Limitations\n    print(\"\\n5. Limitations of this Analysis:\")\n    print(\"   - Limited sample size may affect statistical power\")\n    print(\"   - Correlation doesn't imply causation; other factors may explain observed patterns\")\n    print(\"   - Decision favorability is subjective and may not capture all nuances\")\n    print(\"   - Data quality issues may affect the reliability of findings\")\nelse:\n    print(\"\u274c Cannot generate conclusions without data.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}