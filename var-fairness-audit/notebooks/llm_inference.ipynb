{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udd16 LLM Analysis of VAR Incidents\n\n",
        "**DS 112 Final Project**: VAR Fairness Audit - LLM Extension\n\n",
        "This notebook uses Google Gemini to analyze VAR incident descriptions and compare AI predictions with actual referee decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for Colab\n",
        "!pip install google-generativeai transformers torch seaborn matplotlib pandas scikit-learn plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import google.generativeai as genai\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcca Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the combined VAR data\n",
        "# Upload var_combined.csv to Colab first\n",
        "df = pd.read_csv('var_combined.csv')\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83e\udde0 Google Gemini Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Google Gemini\n",
        "# Add your API key to Colab secrets: Runtime > Manage Sessions > Secrets\n",
        "# Name: GEMINI_API_KEY, Value: your_api_key_here\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "    genai.configure(api_key=api_key)\n",
        "    print(\"\u2705 Gemini API configured successfully\")\n",
        "except Exception as e:\n",
        "    print(\"\u26a0\ufe0f Please add GEMINI_API_KEY to Colab secrets\")\n",
        "    print(f\"Error: {e}\")\n",
        "    \n",
        "    # Alternative: Manual configuration (less secure)\n",
        "    # genai.configure(api_key=\"YOUR_API_KEY_HERE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_var_incident(description, actual_decision):\n",
        "    \"\"\"Use Gemini to analyze VAR incident\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert football referee analyst. Analyze this VAR incident:\n",
        "    \n",
        "    Incident: {description}\n",
        "    Actual Decision: {actual_decision}\n",
        "    \n",
        "    Please provide:\n",
        "    1. Fairness Assessment: FAIR or CONTROVERSIAL (one word)\n",
        "    2. Confidence: 1-10 scale\n",
        "    3. Brief reasoning\n",
        "    \n",
        "    Format:\n",
        "    FAIRNESS: [FAIR/CONTROVERSIAL]\n",
        "    CONFIDENCE: [1-10]\n",
        "    REASONING: [explanation]\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-pro')\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Test with first incident\n",
        "if len(df) > 0:\n",
        "    sample_analysis = analyze_var_incident(\n",
        "        df.iloc[0]['Description'], \n",
        "        df.iloc[0]['Decision']\n",
        "    )\n",
        "    print(\"\ud83d\udd0d Sample LLM Analysis:\")\n",
        "    print(sample_analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83e\udd16 Batch Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze subset of incidents\n",
        "sample_size = min(50, len(df))\n",
        "df_sample = df.head(sample_size).copy()\n",
        "\n",
        "llm_predictions = []\n",
        "llm_confidence = []\n",
        "\n",
        "print(f\"Analyzing {sample_size} incidents...\")\n",
        "\n",
        "for idx, row in df_sample.iterrows():\n",
        "    if idx % 10 == 0:\n",
        "        print(f\"Progress: {idx}/{sample_size}\")\n",
        "    \n",
        "    analysis = analyze_var_incident(row['Description'], row['Decision'])\n",
        "    \n",
        "    # Parse response\n",
        "    fairness = 'FAIR'\n",
        "    confidence = 5\n",
        "    \n",
        "    for line in analysis.split('\\n'):\n",
        "        if 'FAIRNESS:' in line.upper():\n",
        "            fairness = 'CONTROVERSIAL' if 'CONTROVERSIAL' in line.upper() else 'FAIR'\n",
        "        elif 'CONFIDENCE:' in line.upper():\n",
        "            try:\n",
        "                confidence = int(''.join(filter(str.isdigit, line)))\n",
        "                confidence = max(1, min(10, confidence))\n",
        "            except:\n",
        "                confidence = 5\n",
        "    \n",
        "    llm_predictions.append(fairness)\n",
        "    llm_confidence.append(confidence)\n",
        "    \n",
        "    # Small delay\n",
        "    import time\n",
        "    time.sleep(0.5)\n",
        "\n",
        "df_sample['LLM_Prediction'] = llm_predictions\n",
        "df_sample['LLM_Confidence'] = llm_confidence\n",
        "\n",
        "print(\"\u2705 Analysis complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcca Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create binary variables\n",
        "df_sample['Actual_Overturned'] = (df_sample['Decision'] == 'Overturned').astype(int)\n",
        "df_sample['LLM_Controversial'] = (df_sample['LLM_Prediction'] == 'CONTROVERSIAL').astype(int)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(df_sample['Actual_Overturned'], df_sample['LLM_Controversial'])\n",
        "print(f\"\ud83c\udfaf LLM Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\n\ud83d\udccb Classification Report:\")\n",
        "print(classification_report(df_sample['Actual_Overturned'], df_sample['LLM_Controversial']))\n",
        "\n",
        "# Show sample predictions\n",
        "print(\"\\n\ud83d\udd0d Sample Results:\")\n",
        "for i in range(min(5, len(df_sample))):\n",
        "    row = df_sample.iloc[i]\n",
        "    print(f\"Team: {row['Team']}\")\n",
        "    print(f\"Actual: {row['Decision']} | LLM: {row['LLM_Prediction']} (Conf: {row['LLM_Confidence']})\")\n",
        "    print(f\"Description: {row['Description'][:80]}...\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(df_sample['Actual_Overturned'], df_sample['LLM_Controversial'])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['LLM: Fair', 'LLM: Controversial'],\n",
        "            yticklabels=['Actual: Upheld', 'Actual: Overturned'])\n",
        "plt.title('\ud83e\udd16 LLM vs Referee Decisions')\n",
        "plt.ylabel('Actual Decision')\n",
        "plt.xlabel('LLM Prediction')\n",
        "plt.show()\n",
        "\n",
        "# Confidence distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df_sample['LLM_Confidence'], bins=10, alpha=0.7)\n",
        "plt.title('LLM Confidence Distribution')\n",
        "plt.xlabel('Confidence Score')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "pred_counts = df_sample['LLM_Prediction'].value_counts()\n",
        "plt.pie(pred_counts.values, labels=pred_counts.index, autopct='%1.1f%%')\n",
        "plt.title('LLM Predictions')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udccb Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"\ud83c\udfc1 VAR FAIRNESS AUDIT - LLM ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Dataset Statistics:\")\n",
        "print(f\"\u2022 Incidents analyzed: {len(df_sample)}\")\n",
        "print(f\"\u2022 Actual overturns: {df_sample['Actual_Overturned'].sum()} ({df_sample['Actual_Overturned'].mean():.1%})\")\n",
        "print(f\"\u2022 LLM controversial: {df_sample['LLM_Controversial'].sum()} ({df_sample['LLM_Controversial'].mean():.1%})\")\n",
        "\n",
        "print(f\"\\n\ud83e\udd16 LLM Performance:\")\n",
        "print(f\"\u2022 Accuracy: {accuracy:.1%}\")\n",
        "print(f\"\u2022 Avg confidence: {df_sample['LLM_Confidence'].mean():.1f}/10\")\n",
        "\n",
        "if accuracy > 0.7:\n",
        "    print(\"\u2022 \u2705 Good agreement with referees\")\n",
        "elif accuracy > 0.5:\n",
        "    print(\"\u2022 \u26a0\ufe0f Moderate agreement with referees\")\n",
        "else:\n",
        "    print(\"\u2022 \u274c Poor agreement with referees\")\n",
        "\n",
        "print(\"\\n\u2728 Analysis demonstrates AI potential in sports fairness auditing!\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}